{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Affection's annotations and (optionally) linking them to their underlying images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "from eeai.in_out.config import Config\n",
    "from eeai.in_out.datasets.affection import link_image_files_to_dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjust with <u>your input</u> the next cell.**\n",
    "I.e., adapt the `configs/basis.yaml` to point to: \n",
    "- 1. the downloaded [Affection files](https://affective-explanations.org/#dataset): affection\\_{raw, preprocessed}\\_public\\_version\\_0.csv  ([more info](https://github.com/affectivetools/eeai/blob/main/eeai/docs/Affection_Annotations.md))\n",
    "- 2. (optionally,) the underlying image datasets we use in our study (see for expected structure of those directories [here](https://github.com/affectivetools/eeai/blob/main/eeai/docs/Affection_Images.md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = '../configs/basis.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/DATA/Affection/language/version_0/affection_preprocessed_public_version_0.csv\n",
      "\n",
      "you_aaai_affective /home/ubuntu/DATA/Affection/external_images/Emotion_Recognition_AAAI_2016/emotion_dataset\n",
      "coco_affective /home/ubuntu/DATA/Affection/external_images/COCO/2014\n",
      "flickr30k_affective /home/ubuntu/DATA/Affection/external_images/Flickr30k/flickr30k_images/flickr30k_images\n",
      "visual_genome_affective /home/ubuntu/DATA/Affection/external_images/Visual_Genome\n",
      "emotional_machines_affective /home/ubuntu/DATA/Affection/external_images/Emotional_Machines/raw_images\n"
     ]
    }
   ],
   "source": [
    "cfg = Config(config_file)\n",
    "\n",
    "affection_data_file = cfg['data/affection_data/preprocessed_file']  # downloaded per https://affective-explanations.org/#dataset\n",
    "top_img_dirs = cfg['data/image_top_dirs'] # optional, top-directories for COCO, Visual Genome, etc. to link to underlying images\n",
    "\n",
    "print(affection_data_file)\n",
    "print()\n",
    "for key, value in top_img_dirs.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1. Load .csv that contains Affection's annotation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotations loaded: 526749\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(affection_data_file)\n",
    "print('Annotations loaded:', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workerid</th>\n",
       "      <th>image_name</th>\n",
       "      <th>dataset</th>\n",
       "      <th>utterance</th>\n",
       "      <th>emotion</th>\n",
       "      <th>utterance_spelled</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_len</th>\n",
       "      <th>tokens_encoded</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53158</th>\n",
       "      <td>user_900</td>\n",
       "      <td>sadness_1964.jpg</td>\n",
       "      <td>you_aaai_affective</td>\n",
       "      <td>A deceased bird caught in a metal fence makes ...</td>\n",
       "      <td>disgust</td>\n",
       "      <td>a deceased bird caught in a metal fence makes ...</td>\n",
       "      <td>['a', 'deceased', 'bird', 'caught', 'in', 'a',...</td>\n",
       "      <td>17</td>\n",
       "      <td>[1, 17, 7068, 1368, 676, 7, 17, 1507, 2882, 21...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       workerid        image_name             dataset  \\\n",
       "53158  user_900  sadness_1964.jpg  you_aaai_affective   \n",
       "\n",
       "                                               utterance  emotion  \\\n",
       "53158  A deceased bird caught in a metal fence makes ...  disgust   \n",
       "\n",
       "                                       utterance_spelled  \\\n",
       "53158  a deceased bird caught in a metal fence makes ...   \n",
       "\n",
       "                                                  tokens  tokens_len  \\\n",
       "53158  ['a', 'deceased', 'bird', 'caught', 'in', 'a',...          17   \n",
       "\n",
       "                                          tokens_encoded  split  \n",
       "53158  [1, 17, 7068, 1368, 676, 7, 17, 1507, 2882, 21...  train  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2. (optional) Link it with underlying images\n",
    " - Read [here](https://github.com/affectivetools/eeai/blob/main/eeai/docs/Affection_Images.md) for the assumptions we make in the directory structure."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_image_files_to_dataframe(df, top_img_dirs) # now each image can be acccessed in the df['image_file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awe\n",
      "The vivid colors and their placement--such as a blue head--make the bird attractive and garner attention.\n",
      "the vivid colors and their placement such as a blue head make the bird attractive and garner attention\n"
     ]
    }
   ],
   "source": [
    "sample = df.sample(1)\n",
    "print(sample['emotion'].iloc[0]) # dominant-emotion selected\n",
    "print(sample['utterance'].iloc[0]) # raw explanation \n",
    "print(sample['utterance_spelled'].iloc[0]) # spelled-checked++ explanation \n",
    "\n",
    "# Image.open(sample['image_file'].iloc[0]) # Show image. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeai",
   "language": "python",
   "name": "eeai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
